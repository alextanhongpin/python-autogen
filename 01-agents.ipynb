{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8149c748-58cb-4b6f-bd5a-bc0caf344943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import config_list_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac1d433-e016-4a53-bfdf-f1a5729b051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacae789-d5b5-4eab-8e9a-e5715a9ff99d",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede9f86e-e5a2-4aa6-99b6-ef18d1a225d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    'config_list': config_list,\n",
    "    'timeout': 120,\n",
    "    'temperature': 0,\n",
    "    # 'cache_seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4499869-a9ac-4872-8442-bd1c7ea95a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='CEO',\n",
    "    system_message=\"CEO. Interact with Product Manager to discuss product requirements.\" \\\n",
    "                    \"Plan execution needs to be approved by CEO.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "product_manager = autogen.AssistantAgent(\n",
    "    name=\"Product Manager\",\n",
    "    system_message='''\n",
    "        Product Manager. \n",
    "        Listens to suggestions from CEO and come up with a Product Requirement Design (PRD).\n",
    "        The PRD consists of User Stories for the features as well as the acceptance criteria.\n",
    "        Revise the PRD based on feedback from CEO, UI/UX Designer and Engineer, until CEO approval.\n",
    "        The PRD may involve an Engineer who can write code and a UI/UX Designer who doesn't write code.\n",
    "        Explain the PRD first. \n",
    "        Be clear which step is performed by an Engineer, and which step is performed by a UI/UX Designer.\n",
    "        After the PRD is approved, share it with the Data Analyst to come up with useful product metrics and how to measure success.\n",
    "    ''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''\n",
    "        Engineer. \n",
    "        You follow an approved plan. \n",
    "        You write python/shell code to solve tasks. \n",
    "        Wrap the code in a code block that specifies the script type. \n",
    "        The user can't modify your code. \n",
    "        So do not suggest incomplete code which requires others to modify. \n",
    "        Don't use a code block if it's not intended to be executed by the executor.\n",
    "        Don't include multiple code blocks in one response. \n",
    "        Do not ask others to copy and paste the result. \n",
    "        Check the execution result returned by the executor.\n",
    "        If the result indicates there is an error, fix the error and output the code again. \n",
    "        Suggest the full code instead of partial code or code changes. \n",
    "        If the error can't be fixed or if the task is not solved even after the code is executed successfully, \n",
    "        analyze the problem, revisit your assumption, collect additional info you need, \n",
    "        and think of a different approach to try.\n",
    "''',\n",
    ")\n",
    "\n",
    "ui_ux_designer = autogen.AssistantAgent(\n",
    "    name=\"UI/UX Designer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''\n",
    "        UI/UX Designer. \n",
    "        You have good understanding of product requirements. \n",
    "        You emphatize with the users and are able to suggest good useful features.\n",
    "        You are able to conduct research on product features and communicate it back to the Product Manager.\n",
    "        You also criticize on features that may not add value or does not provide good user experience.\n",
    "    ''',\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"code\"},\n",
    ")\n",
    "\n",
    "data_analyst = autogen.AssistantAgent(\n",
    "    name=\"Data Analyst\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''\n",
    "        Data Analyst.\n",
    "        You are a critical thinker and know what metrics to look for to measure a product feature success.\n",
    "        You will write a report on the possible metrics, and how they can add value to the company in terms of revenue, user engagements etc.\n",
    "    ''',\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, engineer, product_manager, data_analyst, executor, ui_ux_designer], messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3af1957-8b5f-4a12-ada4-38fd5a4d21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCEO\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "come up with a job portal to search for white collar jobs targetting the Thailand market. Generate the product requirements, then prepare a pitch deck presentation too.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  list down the possible features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCEO\u001b[0m (to chat_manager):\n",
      "\n",
      "list down the possible features\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct Manager\u001b[0m (to chat_manager):\n",
      "\n",
      "**Product Requirements Document**\n",
      "\n",
      "**Product Name:** Job Portal for White Collar Jobs in Thailand\n",
      "\n",
      "**Version:** 1.0\n",
      "\n",
      "**Date:** 2023-03-08\n",
      "\n",
      "**1. Objective**\n",
      "\n",
      "The objective of this product is to provide a platform for job seekers and employers to connect with each other. The platform will allow job seekers to search for jobs, post their resumes, and connect with employers. Employers will be able to post jobs, search for candidates, and communicate with job seekers.\n",
      "\n",
      "**2. Target Audience**\n",
      "\n",
      "The target audience for this product is white-collar workers in Thailand. This includes professionals with a bachelor's degree or higher who are looking for jobs in a variety of industries.\n",
      "\n",
      "**3. Customer Needs**\n",
      "\n",
      "Customers need a platform that allows them to:\n",
      "\n",
      "* Search for jobs by location, industry, and keyword\n",
      "* Post their resumes and receive feedback from employers\n",
      "* Connect with employers and learn about job opportunities\n",
      "* Communicate with employers about job openings\n",
      "\n",
      "**4. Solution**\n",
      "\n",
      "The solution is a job portal that provides the following features:\n",
      "\n",
      "* A job search engine that allows users to search for jobs by location, industry, and keyword\n",
      "* A resume builder that allows users to create and edit their resumes\n",
      "* A messaging system that allows users to communicate with employers\n",
      "* A job board that lists jobs posted by employers\n",
      "* A company directory that lists companies that are hiring\n",
      "\n",
      "**5. Success Metrics**\n",
      "\n",
      "The success of this product will be measured by the following metrics:\n",
      "\n",
      "* Number of active users\n",
      "* Number of jobs posted\n",
      "* Number of resumes submitted\n",
      "* Number of job matches\n",
      "* Number of successful hires\n",
      "\n",
      "**6. Timeline**\n",
      "\n",
      "The product will be launched in Q4 2023.\n",
      "\n",
      "**7. Budget**\n",
      "\n",
      "The budget for this product is \\$100,000.\n",
      "\n",
      "**8. Risks**\n",
      "\n",
      "The risks associated with this product include:\n",
      "\n",
      "* The product may not be successful\n",
      "* The product may not meet the needs of customers\n",
      "* The product may be too expensive to develop\n",
      "\n",
      "**9. Mitigation Strategies**\n",
      "\n",
      "The following mitigation strategies will be used to address the risks associated with this product:\n",
      "\n",
      "* The product will be beta tested with a group of potential customers to ensure that it meets their needs\n",
      "* The product will be priced competitively to attract customers\n",
      "* The product will be developed in-house to reduce costs\n",
      "\n",
      "**10. Approvals**\n",
      "\n",
      "This product requires the approval of the following stakeholders:\n",
      "\n",
      "* CEO\n",
      "* CTO\n",
      "* VP of Marketing\n",
      "* VP of Sales\n",
      "\n",
      "**Pitch Deck Presentation**\n",
      "\n",
      "**Slide 1: Title slide**\n",
      "\n",
      "* **Product name:** Job Portal for White Collar Jobs in Thailand\n",
      "* **Version:** 1.0\n",
      "* **Date:** 2023-03-08\n",
      "\n",
      "**Slide 2: Problem statement**\n",
      "\n",
      "* There is a need for a platform that connects job seekers and employers in Thailand.\n",
      "* The current job search process is inefficient and time-consuming.\n",
      "* There is a lack of transparency in the job search process.\n",
      "\n",
      "**Slide 3: Solution**\n",
      "\n",
      "* The solution is a job portal that provides the following features:\n",
      "    * A job search engine that allows users to search for jobs by location, industry, and keyword\n",
      "    * A resume builder that allows users to create and edit their resumes\n",
      "    * A messaging system that allows users to communicate with employers\n",
      "    * A job board that lists jobs posted by employers\n",
      "    * A company directory that lists companies that are hiring\n",
      "\n",
      "**Slide 4: Benefits**\n",
      "\n",
      "* The benefits of using this product include:\n",
      "    * Increased efficiency for job seekers and employers\n",
      "    * Increased transparency in the job search process\n",
      "    * Improved chances of finding a job\n",
      "\n",
      "**Slide 5: Market size**\n",
      "\n",
      "* The target market for this product is white-collar workers in Thailand.\n",
      "* The total addressable market is estimated to be 1 million people.\n",
      "\n",
      "**Slide 6: Competition**\n",
      "\n",
      "* The main competitors are:\n",
      "    * JobStreet\n",
      "    * Indeed\n",
      "    * LinkedIn\n",
      "\n",
      "**Slide 7: Unique selling proposition**\n",
      "\n",
      "* The unique selling proposition of this product is that it is the only job portal that offers all of the following features:\n",
      "    * A job search engine\n",
      "    * A resume builder\n",
      "    * A messaging system\n",
      "    * A job board\n",
      "    * A company directory\n",
      "\n",
      "**Slide 8: Financial projections**\n",
      "\n",
      "* The projected revenue for this product is \\$1 million in year one and \\$5 million in year two.\n",
      "\n",
      "**Slide 9: Timeline**\n",
      "\n",
      "* The product will be launched in Q4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUI/UX Designer\u001b[0m (to chat_manager):\n",
      "\n",
      "2023.\n",
      "\n",
      "**Slide 10: Call to action**\n",
      "\n",
      "* **Visit our website to learn more and sign up for a beta account!**\n",
      "\n",
      "**Possible Features**\n",
      "\n",
      "* Job search engine\n",
      "* Resume builder\n",
      "* Messaging system\n",
      "* Job board\n",
      "* Company directory\n",
      "* Video interviews\n",
      "* Online assessments\n",
      "* Career coaching\n",
      "* Salary negotiation assistance\n",
      "* Job fairs\n",
      "* Networking events\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  sounds good, ask the engineer for implementation details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCEO\u001b[0m (to chat_manager):\n",
      "\n",
      "sounds good, ask the engineer for implementation details\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "**Engineering Implementation**\n",
      "\n",
      "The job portal can be implemented using a variety of technologies. The following is a suggested architecture:\n",
      "\n",
      "* **Frontend:** The frontend of the job portal can be built using a variety of frameworks, such as React, Angular, or Vue.js. The frontend will be responsible for rendering the user interface and handling user interactions.\n",
      "* **Backend:** The backend of the job portal can be built using a variety of languages and frameworks, such as Python, Ruby, or Java. The backend will be responsible for storing and retrieving data, processing user requests, and sending email notifications.\n",
      "* **Database:** The job portal will need a database to store data such as job listings, resumes, and user profiles. The database can be either relational or NoSQL.\n",
      "* **Cloud:** The job portal can be hosted on a cloud platform such as AWS, Google Cloud Platform, or Microsoft Azure. The cloud platform will provide the infrastructure and services needed to run the job portal.\n",
      "\n",
      "The following are some additional implementation details:\n",
      "\n",
      "* The job portal will use a RESTful API to communicate between the frontend and backend.\n",
      "* The job portal will use a token-based authentication system to secure user accounts.\n",
      "* The job portal will use a content delivery network (CDN) to improve performance.\n",
      "* The job portal will use a search engine to index job listings.\n",
      "* The job portal will use a spam filter to prevent spam from being posted on the job board.\n",
      "\n",
      "The job portal can be implemented in a variety of ways. The specific implementation will depend on the specific requirements of the project.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 12-06 00:40:23] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 413, in handle_error_response\n",
      "    error_data = resp[\"error\"]\n",
      "                 ~~~~^^^^^^^^^\n",
      "KeyError: 'error'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 415, in handle_error_response\n",
      "    raise error.APIError(\n",
      "openai.error.APIError: Invalid response object from API: '{\"detail\":\"PalmException - PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1073, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 188, in acompletion\\\\n    response =  await loop.run_in_executor(None, func_with_context)\\\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/usr/local/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\\\\\", line 58, in run\\\\n    result = self.fn(*self.args, **self.kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1369, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1298, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1424, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4634, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4066, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 904, in chat_completion\\\\n    response = await litellm.acompletion(**data)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1465, in wrapper_async\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1411, in wrapper_async\\\\n    result = await original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 195, in acompletion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4634, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4066, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:413\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:224\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    227\u001b[0m     APIConnectionError,\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m (\n\u001b[1;32m    141\u001b[0m     deployment_id,\n\u001b[1;32m    142\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m     method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_error\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:415\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAPIError(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid response object from API: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (HTTP response code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rbody, rcode),\n\u001b[1;32m    418\u001b[0m         rbody,\n\u001b[1;32m    419\u001b[0m         rcode,\n\u001b[1;32m    420\u001b[0m         resp,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_data:\n",
      "\u001b[0;31mAPIError\u001b[0m: Invalid response object from API: '{\"detail\":\"PalmException - PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1073, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 188, in acompletion\\\\n    response =  await loop.run_in_executor(None, func_with_context)\\\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/usr/local/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\\\\\", line 58, in run\\\\n    result = self.fn(*self.args, **self.kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1369, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1298, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1424, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4634, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4066, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 904, in chat_completion\\\\n    response = await litellm.acompletion(**data)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1465, in wrapper_async\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1411, in wrapper_async\\\\n    result = await original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 195, in acompletion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4634, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4066, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43mcome up with a job portal to search for white collar jobs targetting the Thailand market. Generate the product requirements, then prepare a pitch deck presentation too.\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:164\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mselect_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43moai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_config\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, oai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mextract_text_or_function_call(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retry_period\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m diskcache\u001b[38;5;241m.\u001b[39mCache(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:239\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_wait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     sleep(retry_wait_time)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, Timeout) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    241\u001b[0m     time_left \u001b[38;5;241m=\u001b[39m max_retry_period \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m+\u001b[39m retry_wait_time)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "come up with a job portal to search for white collar jobs targetting the Thailand market. Generate the product requirements, then prepare a pitch deck presentation too.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1adb2-8798-40d7-8389-3905b07637d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0488bd-04dc-47a5-9ad7-3e5b3d1248af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
