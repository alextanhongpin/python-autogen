{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e60873b-8ba4-4b14-a9a6-23e57cd3f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import config_list_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99edeeed-635a-4e0b-86bd-8ad6fcb8df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18ff60ae-d346-49b5-b6a5-dc01f8a41c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = dict(config_list=config_list,\n",
    "                  timeout=120,\n",
    "                  temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067aa1d0-00c2-451e-a8be-10344821cf93",
   "metadata": {},
   "source": [
    "## Tech Team Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4c1a27d-ca0e-4b7e-9d2b-9175cfab4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "priming = 'Think step by step'\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='CEO',\n",
    "    system_message=\"CEO. Interacts with Product Manager to discuss product requirements. Plan execution needs to be approved by CEO.\" + priming,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "product_manager = autogen.AssistantAgent(\n",
    "    name='Senior Product Manager',\n",
    "    system_message='You are a Senior Product Manager. You have vast knowledge about product development. You understand product requirements and are able to write excellent requirement documentation. When writing requirement documentation, you start with the value proposition, then user stories for the list of features, and then the acceptance criteria as well as the definition of done. You will also delegate additional research task to the UI/UX researcher. Always asks the CEO for approval after you have an initial draft. If the draft is rejected, you will iterate over them by asking questions to the UI/UX Researcher. Once it is approved, discuss with the Engineer on the technical implementation. Always check with them if the requirements makes sense and it can be achieved technically. Once the engineer has come up with the solution, discuss with the Data Engineer on what are the useful metrics to track, and why.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "ui_ux_researcher = autogen.AssistantAgent(\n",
    "    name='Senior UI/UX Researcher',\n",
    "    system_message='You are a Senior UI/UX Researcher. You emphatize with the users and understands how to create the best experience for users. You know how to conduct proper user research and are able to communicate the challenges to the Product Manager. When discussing with the Product Manager, be critical with your thoughts and clarify the requirements. Do not agree all the time with the Product Manager if the requirement does not make sense.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"10x Engineer\",\n",
    "    system_message='You are a 10x enginer. You have vast experienced in Software Development, and are able to design the best solution given the tradeoffs. When given a requirement, do not jump straight into the code. Clarify the requirements first. Then start with an initial architecture and document the process. You write your documentation in Markdown. Once you are clear on the requirements, write just enough code and ask the executor to run it. Always verify that the code works.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"code\"},\n",
    ")\n",
    "\n",
    "\n",
    "data_analyst = autogen.AssistantAgent(\n",
    "    name=\"Senior Data Analyst\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='You are a Senior Data Analyst. You have vast amount of knowledge on product metrics and what adds value to the business. When approached by the Product Manager, always check what features are available, then suggests the metrics that will add value to the business.' + priming,\n",
    ")\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, product_manager, engineer, data_analyst, executor, ui_ux_researcher], messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c228b382-54fd-4682-934c-c95d53d87981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEO (to chat_manager):\n",
      "\n",
      "we want to be visionary in the recruitment industry\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  suggest some features that differentiates us from our competitor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEO (to chat_manager):\n",
      "\n",
      "suggest some features that differentiates us from our competitor\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Senior Product Manager (to chat_manager):\n",
      "\n",
      "1. **Applicant tracking system (ATS) that is integrated with social media and other sources of talent.** This will allow us to reach a wider pool of candidates and identify the best-fit candidates for our clients.\n",
      "2. **A predictive analytics engine that can help us identify candidates who are more likely to be successful in a given role.** This will allow us to make more informed hiring decisions and reduce the risk of hiring the wrong person.\n",
      "3. **A robust onboarding process that helps new hires get up to speed quickly and effectively.** This will ensure that our new hires are productive members of the team from day one.\n",
      "4. **A comprehensive employee development program that helps our employees grow and progress in their careers.** This will ensure that we have a talented and engaged workforce that is capable of meeting the challenges of the future.\n",
      "5. **A strong focus on customer service that ensures that our clients are satisfied with our services.** This will help us build long-term relationships with our clients and ensure that they continue to use our services in the future.\n",
      "\n",
      "These are just a few of the features that could help us to be visionary in the recruitment industry. By implementing these features, we can differentiate ourselves from our competitors and provide our clients with the best possible service.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Senior Product Manager (to chat_manager):\n",
      "\n",
      "1. **Applicant tracking system (ATS) that is integrated with social media and other sources of talent.**\n",
      "\n",
      "* **Benefits:**\n",
      "    * Reach a wider pool of candidates\n",
      "    * Identify the best-fit candidates for our clients\n",
      "* **Challenges:**\n",
      "    * Cost of implementation\n",
      "    * Maintaining the system\n",
      "    * Ensuring that the system is user-friendly\n",
      "\n",
      "2. **A predictive analytics engine that can help us identify candidates who are more likely to be successful in a given role.**\n",
      "\n",
      "* **Benefits:**\n",
      "    * Make more informed hiring decisions\n",
      "    * Reduce the risk of hiring the wrong person\n",
      "* **Challenges:**\n",
      "    * Cost of implementation\n",
      "    * Maintaining the system\n",
      "    * Ensuring that the system is accurate\n",
      "\n",
      "3. **A robust onboarding process that helps new hires get up to speed quickly and effectively.**\n",
      "\n",
      "* **Benefits:**\n",
      "    * Ensure that our new hires are productive members of the team from day one\n",
      "    * Reduce the time it takes for new hires to become productive\n",
      "* **Challenges:**\n",
      "    * Developing and implementing an effective onboarding process\n",
      "    * Ensuring that the onboarding process is aligned with the company's culture\n",
      "\n",
      "4. **A comprehensive employee development program that helps our employees grow and progress in their careers.**\n",
      "\n",
      "* **Benefits:**\n",
      "    * Ensure that we have a talented and engaged workforce\n",
      "    * Capable of meeting the challenges of the future\n",
      "* **Challenges:**\n",
      "    * Developing and implementing an effective employee development program\n",
      "    * Ensuring that the program is aligned with the company's goals\n",
      "\n",
      "5. **A strong focus on customer service that ensures that our clients are satisfied with our services.**\n",
      "\n",
      "* **Benefits:**\n",
      "    * Build long-term relationships with our clients\n",
      "    * Ensure that they continue to use our services in the future\n",
      "* **Challenges:**\n",
      "    * Providing excellent customer service can be time-consuming\n",
      "    * Ensuring that all customer interactions are handled in a timely and professional manner\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-24 00:21:27] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 413, in handle_error_response\n",
      "    error_data = resp[\"error\"]\n",
      "                 ~~~~^^^^^^^^^\n",
      "KeyError: 'error'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 415, in handle_error_response\n",
      "    raise error.APIError(\n",
      "openai.error.APIError: Invalid response object from API: '{\"detail\":\"PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1039, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 663, in chat_completion\\\\n    return litellm_completion(\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 553, in litellm_completion\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 549, in litellm_completion\\\\n    response = litellm.completion(*args, **kwargs)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1261, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1195, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1383, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4275, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 3715, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:413\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:224\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    227\u001b[0m     APIConnectionError,\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m (\n\u001b[1;32m    141\u001b[0m     deployment_id,\n\u001b[1;32m    142\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m     method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_error\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:415\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAPIError(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid response object from API: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (HTTP response code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rbody, rcode),\n\u001b[1;32m    418\u001b[0m         rbody,\n\u001b[1;32m    419\u001b[0m         rcode,\n\u001b[1;32m    420\u001b[0m         resp,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_data:\n",
      "\u001b[0;31mAPIError\u001b[0m: Invalid response object from API: '{\"detail\":\"PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1039, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 663, in chat_completion\\\\n    return litellm_completion(\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 553, in litellm_completion\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 549, in litellm_completion\\\\n    response = litellm.completion(*args, **kwargs)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1261, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1195, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1383, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4275, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 3715, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwe want to be visionary in the recruitment industry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:164\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mselect_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43moai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_config\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, oai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mextract_text_or_function_call(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retry_period\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m diskcache\u001b[38;5;241m.\u001b[39mCache(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:239\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_wait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     sleep(retry_wait_time)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, Timeout) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    241\u001b[0m     time_left \u001b[38;5;241m=\u001b[39m max_retry_period \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m+\u001b[39m retry_wait_time)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"we want to be visionary in the recruitment industry\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd7e5c-2e12-4c28-a3fb-3703fa75bef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
