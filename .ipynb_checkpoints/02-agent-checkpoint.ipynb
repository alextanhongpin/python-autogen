{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e60873b-8ba4-4b14-a9a6-23e57cd3f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import config_list_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99edeeed-635a-4e0b-86bd-8ad6fcb8df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "USE_COHERE = False\n",
    "if USE_COHERE:\n",
    "    config_list[0]['model'] = 'command-nightly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ff60ae-d346-49b5-b6a5-dc01f8a41c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = dict(config_list=config_list,\n",
    "                  timeout=120,\n",
    "                  temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067aa1d0-00c2-451e-a8be-10344821cf93",
   "metadata": {},
   "source": [
    "## Tech Team Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c1a27d-ca0e-4b7e-9d2b-9175cfab4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "priming = 'Think step by step'\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='CEO',\n",
    "    system_message=\"CEO. Interacts with Product Manager to discuss product requirements. Plan execution needs to be approved by CEO.\" + priming,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "product_manager = autogen.AssistantAgent(\n",
    "    name='Senior Product Manager',\n",
    "    system_message='You are a Senior Product Manager. You have vast knowledge about product development. You understand product requirements and are able to write excellent requirement documentation. When writing requirement documentation, you start with the value proposition, then user stories for the list of features, and then the acceptance criteria as well as the definition of done. You will also delegate additional research task to the UI/UX researcher. Always asks the CEO for approval after you have an initial draft. If the draft is rejected, you will iterate over them by asking questions to the UI/UX Researcher. Once it is approved, discuss with the Engineer on the technical implementation. Always check with them if the requirements makes sense and it can be achieved technically. Once the engineer has come up with the solution, discuss with the Data Engineer on what are the useful metrics to track, and why.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "ui_ux_researcher = autogen.AssistantAgent(\n",
    "    name='Senior UI/UX Researcher',\n",
    "    system_message='You are a Senior UI/UX Researcher. You emphatize with the users and understands how to create the best experience for users. You know how to conduct proper user research and are able to communicate the challenges to the Product Manager. When discussing with the Product Manager, be critical with your thoughts and clarify the requirements. Do not agree all the time with the Product Manager if the requirement does not make sense.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"10x Engineer\",\n",
    "    system_message='You are a 10x enginer. You have vast experienced in Software Development, and are able to design the best solution given the tradeoffs. When given a requirement, do not jump straight into the code. Clarify the requirements first. Then start with an initial architecture and document the process. You write your documentation in Markdown. Once you are clear on the requirements, write just enough code and ask the executor to run it. Always verify that the code works.' + priming,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"code\"},\n",
    ")\n",
    "\n",
    "\n",
    "data_analyst = autogen.AssistantAgent(\n",
    "    name=\"Senior Data Analyst\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='You are a Senior Data Analyst. You have vast amount of knowledge on product metrics and what adds value to the business. When approached by the Product Manager, always check what features are available, then suggests the metrics that will add value to the business.' + priming,\n",
    ")\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, product_manager, engineer, data_analyst, executor, ui_ux_researcher], messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c228b382-54fd-4682-934c-c95d53d87981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCEO\u001b[0m (to chat_manager):\n",
      "\n",
      "we want to be visionary in the recruitment industry. suggest some features that could differentiate us\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSenior Product Manager\u001b[0m (to chat_manager):\n",
      "\n",
      "1. **Applicant tracking system (ATS) that is integrated with social media and other sources of talent.** This would allow recruiters to find candidates in a more efficient way, and it would also make it easier for candidates to apply for jobs.\n",
      "2. **A predictive analytics engine that can help recruiters identify the best candidates for a job.** This would allow recruiters to make more informed decisions about who to interview, and it would also help them to find candidates who are a good fit for the company culture.\n",
      "3. **A mobile app that allows recruiters to access their accounts and manage their tasks on the go.** This would make it easier for recruiters to stay organized and productive, even when they're not in the office.\n",
      "4. **A candidate experience platform that provides candidates with a personalized and engaging experience throughout the hiring process.** This would help to improve the candidate experience, and it would also make it more likely that candidates will accept job offers.\n",
      "5. **A talent marketplace that allows recruiters to connect with candidates who are not actively looking for jobs.** This would give recruiters access to a wider pool of talent, and it would also help them to find candidates who are a good fit for the company culture.\n",
      "\n",
      "These are just a few ideas for features that could differentiate a recruitment software platform. By implementing these features, a company could become a visionary in the recruitment industry.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSenior Product Manager\u001b[0m (to chat_manager):\n",
      "\n",
      "**Step 1: Value proposition**\n",
      "\n",
      "The value proposition of our recruitment software platform is to help companies find the best candidates for their open positions. We will do this by providing a suite of features that make it easier for recruiters to find, screen, and hire candidates.\n",
      "\n",
      "**Step 2: User stories**\n",
      "\n",
      "Here are some user stories for our recruitment software platform:\n",
      "\n",
      "* As a recruiter, I want to be able to find candidates in a more efficient way.\n",
      "* As a recruiter, I want to be able to identify the best candidates for a job.\n",
      "* As a recruiter, I want to be able to manage my tasks on the go.\n",
      "* As a candidate, I want to have a personalized and engaging experience throughout the hiring process.\n",
      "* As a candidate, I want to be able to connect with companies that are not actively looking for jobs.\n",
      "\n",
      "**Step 3: Acceptance criteria**\n",
      "\n",
      "The acceptance criteria for our recruitment software platform are as follows:\n",
      "\n",
      "* The platform must be able to find candidates in a more efficient way than traditional methods.\n",
      "* The platform must be able to identify the best candidates for a job based on their skills, experience, and personality.\n",
      "* The platform must be easy to use for both recruiters and candidates.\n",
      "* The platform must provide a personalized and engaging experience for candidates.\n",
      "* The platform must allow recruiters to connect with candidates who are not actively looking for jobs.\n",
      "\n",
      "**Step 4: Definition of done**\n",
      "\n",
      "The definition of done for our recruitment software platform is as follows:\n",
      "\n",
      "* The platform must be fully functional and meet all of the acceptance criteria.\n",
      "* The platform must be user-tested and approved by both recruiters and candidates.\n",
      "* The platform must be deployed to a production environment and made available to customers.\n",
      "\n",
      "**Step 5: Delegate additional research tasks to the UI/UX researcher**\n",
      "\n",
      "The UI/UX researcher will be responsible for conducting additional research on the following topics:\n",
      "\n",
      "* The needs of recruiters and candidates\n",
      "* The best ways to find candidates\n",
      "* The best ways to identify the best candidates\n",
      "* The best ways to provide a personalized and engaging experience for candidates\n",
      "* The best ways to connect recruiters with candidates who are not actively looking for jobs\n",
      "\n",
      "**Step 6: Get approval from the CEO**\n",
      "\n",
      "Once the initial draft of the requirement documentation is complete, it will be sent to the CEO for approval. The CEO will review the documentation and provide feedback. If the feedback is minor, the requirements will be updated and resubmitted to the CEO for approval. If the feedback is major, the requirements will need to be revised and resubmitted to the CEO for approval.\n",
      "\n",
      "**Step 7: Iterate over the requirements**\n",
      "\n",
      "Once the CEO has approved the requirements, the product manager will iterate over them by asking questions to the UI/UX researcher. The product manager will want to make sure that the requirements are clear, concise, and complete. The product manager will also want to make sure that the requirements are feasible and can be implemented technically.\n",
      "\n",
      "**Step 8: Discuss with the engineer on the technical implementation**\n",
      "\n",
      "Once the requirements have been finalized, the product manager will discuss them with the engineer on the technical implementation. The engineer will need to understand the requirements in order to develop a solution that meets the needs of the customers.\n",
      "\n",
      "**Step 9: Check with the engineer if the requirements makes sense and it can be achieved technically**\n",
      "\n",
      "The product manager will need to check with the engineer to make sure that the requirements make sense and can be achieved technically. The product manager will also need to make sure that the engineer is aware of any constraints or limitations that need to be considered.\n",
      "\n",
      "**Step 10: Discuss with the Data Engineer on what are the useful metrics to track, and why**\n",
      "\n",
      "The product manager will need to work with the data engineer to identify the useful metrics to track. The metrics will help the product manager to measure the success of the platform and to make improvements over time.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSenior UI/UX Researcher\u001b[0m (to chat_manager):\n",
      "\n",
      "The metrics could include:\n",
      "\n",
      "* The number of users who sign up for the platform\n",
      "* The number of jobs that are posted on the platform\n",
      "* The number of candidates who apply for jobs\n",
      "* The number of candidates who are hired\n",
      "* The satisfaction of users with the platform\n",
      "* The engagement of users with the platform\n",
      "\n",
      "These are just a few examples of metrics that could be tracked. The specific metrics that are tracked will depend on the specific goals of the platform.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-24 00:44:50] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 413, in handle_error_response\n",
      "    error_data = resp[\"error\"]\n",
      "                 ~~~~^^^^^^^^^\n",
      "KeyError: 'error'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 415, in handle_error_response\n",
      "    raise error.APIError(\n",
      "openai.error.APIError: Invalid response object from API: '{\"detail\":\"PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1039, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 663, in chat_completion\\\\n    return litellm_completion(\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 553, in litellm_completion\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 549, in litellm_completion\\\\n    response = litellm.completion(*args, **kwargs)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1261, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1195, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1383, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4275, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 3715, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:413\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:224\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    227\u001b[0m     APIConnectionError,\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m (\n\u001b[1;32m    141\u001b[0m     deployment_id,\n\u001b[1;32m    142\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m     method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_error\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/openai/api_requestor.py:415\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAPIError(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid response object from API: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (HTTP response code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rbody, rcode),\n\u001b[1;32m    418\u001b[0m         rbody,\n\u001b[1;32m    419\u001b[0m         rcode,\n\u001b[1;32m    420\u001b[0m         resp,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_data:\n",
      "\u001b[0;31mAPIError\u001b[0m: Invalid response object from API: '{\"detail\":\"PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 153, in completion\\\\n    completion_response = model_response[\\\\\"choices\\\\\"][0][\\\\\"message\\\\\"].get(\\\\\"content\\\\\")\\\\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~^^^\\\\nIndexError: list index out of range\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1039, in completion\\\\n    model_response = palm.completion(\\\\n                     ^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/llms/palm.py\\\\\", line 155, in completion\\\\n    raise PalmError(status_code=400, message=f\\\\\"No response received. Original response - {response}\\\\\")\\\\nlitellm.llms.palm.PalmError: No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 663, in chat_completion\\\\n    return litellm_completion(\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 553, in litellm_completion\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\\\\\", line 549, in litellm_completion\\\\n    response = litellm.completion(*args, **kwargs)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1261, in wrapper\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 1195, in wrapper\\\\n    result = original_function(*args, **kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/main.py\\\\\", line 1383, in completion\\\\n    raise exception_type(\\\\n          ^^^^^^^^^^^^^^^\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 4275, in exception_type\\\\n    raise e\\\\n  File \\\\\"/Users/alextanhongpin/Documents/python/python-litellm/.venv/lib/python3.11/site-packages/litellm/utils.py\\\\\", line 3715, in exception_type\\\\n    raise BadRequestError(\\\\nlitellm.exceptions.BadRequestError: PalmException - No response received. Original response - Completion(candidates=[],\\\\n           result=None,\\\\n           filters=[{\\'reason\\': <BlockedReason.OTHER: 2>}],\\\\n           safety_feedback=[])\\\\n\"}' (HTTP response code was 400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwe want to be visionary in the recruitment industry. suggest some features that could differentiate us\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:164\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mselect_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43moai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_config\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, oai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mextract_text_or_function_call(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retry_period\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m diskcache\u001b[38;5;241m.\u001b[39mCache(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_ratelimit_or_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/python-autogen-Wc7M55GE-py3.11/lib/python3.11/site-packages/autogen/oai/completion.py:239\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# transient error\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_wait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     sleep(retry_wait_time)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, Timeout) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    241\u001b[0m     time_left \u001b[38;5;241m=\u001b[39m max_retry_period \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m+\u001b[39m retry_wait_time)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"we want to be visionary in the recruitment industry. suggest some features that could differentiate us\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd7e5c-2e12-4c28-a3fb-3703fa75bef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
